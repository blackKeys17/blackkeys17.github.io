<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on BlackKeys17</title>
    <link>http://www.example.com/posts/</link>
    <description>Recent content in Posts on BlackKeys17</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 18 Sep 2025 12:15:23 +0100</lastBuildDate>
    <atom:link href="http://www.example.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A mathematical introduction to feedforward neural networks</title>
      <link>http://www.example.com/posts/neural_net_maths/</link>
      <pubDate>Thu, 18 Sep 2025 12:15:23 +0100</pubDate>
      <guid>http://www.example.com/posts/neural_net_maths/</guid>
      <description>&lt;p&gt;The introduction of feedforward neural networks and their application to early problems such as recognising handwritten digits from the MNIST dataset was a landmark achievement. Despite their limited standalone use today, they still remain the backbone of various models of wide range of modern architectures Incredibly, the backpropagation algorithm, which only started gaining traction in the 1980&amp;rsquo;s, is still used when updating the billions of trainable parameters in the massive LLMs we see today.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
