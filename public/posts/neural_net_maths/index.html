<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  A mathematical introduction to feedforward neural networks · BlackKeys17
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Ben Chang">
<meta name="description" content="The introduction of feedforward neural networks and their application to early problems such as recognising handwritten digits from the MNIST dataset was a landmark achievement. Despite their limited standalone use today, they still remain the backbone of various models of wide range of modern architectures Incredibly, the backpropagation algorithm, which only started gaining traction in the 1980&rsquo;s, is still used when updating the billions of trainable parameters in the massive LLMs we see today.">
<meta name="keywords" content="blog">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="A mathematical introduction to feedforward neural networks">
  <meta name="twitter:description" content="The introduction of feedforward neural networks and their application to early problems such as recognising handwritten digits from the MNIST dataset was a landmark achievement. Despite their limited standalone use today, they still remain the backbone of various models of wide range of modern architectures Incredibly, the backpropagation algorithm, which only started gaining traction in the 1980’s, is still used when updating the billions of trainable parameters in the massive LLMs we see today.">

<meta property="og:url" content="http://www.example.com/posts/neural_net_maths/">
  <meta property="og:site_name" content="BlackKeys17">
  <meta property="og:title" content="A mathematical introduction to feedforward neural networks">
  <meta property="og:description" content="The introduction of feedforward neural networks and their application to early problems such as recognising handwritten digits from the MNIST dataset was a landmark achievement. Despite their limited standalone use today, they still remain the backbone of various models of wide range of modern architectures Incredibly, the backpropagation algorithm, which only started gaining traction in the 1980’s, is still used when updating the billions of trainable parameters in the massive LLMs we see today.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-18T12:15:23+01:00">
    <meta property="article:modified_time" content="2025-09-18T12:15:23+01:00">




<link rel="canonical" href="http://www.example.com/posts/neural_net_maths/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.022594d625780e2edf64581b893d32cb35c11de5d88953ea4ad3c2e45451e214.css" integrity="sha256-AiWU1iV4Di7fZFgbiT0yyzXBHeXYiVPqStPC5FRR4hQ=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/img/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <link rel="stylesheet" href="/css/custom.css">
<nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://www.example.com/">
      BlackKeys17
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://www.example.com/posts/neural_net_maths/">
              A mathematical introduction to feedforward neural networks
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2025-09-18T12:15:23&#43;01:00">
                September 18, 2025
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              20-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <p>The introduction of feedforward neural networks and their application to early problems such as recognising handwritten digits from the MNIST dataset was a landmark achievement. Despite their limited standalone use today, they still remain the backbone of various models of wide range of modern architectures Incredibly, the backpropagation algorithm, which only started gaining traction in the 1980&rsquo;s, is still used when updating the billions of trainable parameters in the massive LLMs we see today.</p>
<p>This post aims to give a mathematical description of what a feedforward neural network is, and how it can be trained. It assumes some basic knowledge in single and multivariable calculus, including the chain rule for partial derivatives, and some basic knowledge of matrices.</p>
<hr>
<h2 id="defining-the-problem">
  Defining the problem
  <a class="heading-link" href="#defining-the-problem">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>We first need to know what this network is even meant to do! For now, we will focus on the classic task of recognising handwritten greyscale digits as an example.</p>
<p>More generally, we focus on tasks which involve <strong>classification</strong>. Given some inputs, we want to be able to classify them into different categories based on their attributes.</p>
<p>The images come from a dataset called <strong>MNIST</strong>. Without going into any of its history, it is a dataset consisting of handwritten digits from 0-9 on a 28x28 grid, each image coming with a label telling us which number the pixels show. The images and labels are split into 2 separate datasets, the first to be used for training the model, and the second to be used for testing how well the model can classify drawings of digits it has never seen before.</p>
<p>The images are represented as a 28x28 grid of numbers between 0 and 255 inclusive, where each number refers to the intensity of a particular pixel on the grid.</p>
<p>The examples below shows how the pixel values match their corresponding intensities:</p>
<p><img src="figure_1.png" alt="Placeholder"></p>
<p><img src="figure_2.png" alt="Placeholder"></p>
<p>Here, I&rsquo;ve already normalised all of the values by dividing them by 256 which is how they would actually be passed into the network (but mainly so they fit inside the image).</p>
<p>Before we see how a neural network attempts to &lsquo;solve&rsquo; this classification problem, it is important to define what the neural network even is.</p>
<hr>
<h2 id="the-neuron">
  The neuron
  <a class="heading-link" href="#the-neuron">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>The neuron is the most fundamental &ldquo;building block&rdquo; of a neural network. It takes some inputs, and maps them to a single output, which is somehow meant to &ldquo;encode&rdquo; some sort of information about the inputs.</p>
<p>Each neuron has associated with it $n$ weights, which we denote $w_1,&hellip;,w_n$ for now, where $n$ is the number of inputs to the neuron. It also has a single bias, $b$.</p>
<p>We can collectively refer to the weights and biases as parameters. When we train a neural network, we are actually just modifying the weights and biases of the many neurons in the network to achieve some sort of desired behaviour on inputs feeded into the network.</p>
<p>We define the function $\sigma(z)$ as follows:<br>
</p>
$$\sigma(z)=\frac{1}{1+e^{-z}}$$<p>Given some $n$ inputs $x_1,&hellip;,x_n$, the output $a$ of the neuron is:<br>
</p>
$$a=\sigma(w_1x_1+...+w_nx_n+b)$$<p>Or more compactly:<br>
</p>
$$a=\sigma\left(\sum{w_ix_i}+b\right)$$<p>where the summand is taken to be over all the inputs.</p>
<p>The visual representation below is commonly used:</p>
<p><img src="figure_3.png" alt="alt text"></p>
<p>With networks containing loads of these neurons (and therefore loads of connections), writing out the weights on each individual connection will clutter up the image, so they are usually omitted.</p>
<p>This representation allows us to give a nice graph-like representation of the data flowing through a neural network when we actually start constructing them.</p>
<p>From just looking at the formula, there is quite a bit to break down!</p>
<p>WIthout worrying about the $\sigma$ for the moment, the inside sum is a weighted sum of the inputs $x_i$, with the various $w_i$. The bias term is then added on, leaving us with what we call the <strong>weighted input</strong> to the neuron:</p>
<p>$z=\sum{w_ix_i} + b$</p>
<p>This &ldquo;intermediate&rdquo; quantity turns out to be incredibly useful when we start training the network, but more on that later&hellip;</p>
<p>After that, we apply the function $\sigma$ to this weighted input, leaving us with the final output of the neuron, $a=\sigma(z)$.</p>
<p>In this context, the sigmoid function acts as an <strong>activation function</strong>. There are many other possibilities for the activation function, such as hyperbolic tangent or ReLU, which have generally seen better results in practice, but for now, we will just stick with the sigmoid.</p>
<p>The function $\sigma(z)$ we defined above is the sigmoid function. There are a few useful and important properties we can deduce about it. It may help to look at a graph sketch of the output of the sigmoid vs its input, which is shown below:</p>
<p><img src="figure_4.png" alt="Placeholder"></p>
<p>It isn&rsquo;t too hard to show it is increasing, and we can see that its gradient <em>vanishes</em> as $z$ becomes either very large or very small. This is actually really important, as we see later.</p>
<p>It can be verified algebraically that we have the relation:</p>
<p>$\sigma&rsquo;(z)=\sigma(z)(1-\sigma(z))$</p>
<p>(where &rsquo; stands for differentiation w.r.t. the input as usual), which turns out to help when motivating the definition of <strong>cross-entropy loss</strong>. But again, more on that later&hellip;</p>
<p>It can also be easily verified that $\sigma(z)$ is bounded between 0 and 1, and is close to 0 for very negative values of $z$, and close to 1 for very positive values of $z$.</p>
<hr>
<h2 id="defining-the-network">
  Defining the network
  <a class="heading-link" href="#defining-the-network">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Now we are ready to define what a neural network is.</p>
<p>A neural network has an <strong>input layer</strong> and an <strong>output layer</strong>. When using the network as a classifier after training, we can think of the network as a function which maps inputs (placed into the input layer) to outputs (from the outputs layer). Between them, there are <strong>hidden layers</strong>. They are placed in the following arrangement:</p>
<p><img src="figure_5.png" alt="alt text"></p>
<p>The number of neurons in each layer will obviously be different in a neural network for classifying MNIST images (it would have 784 inputs and 10 outputs for example).</p>
<p>Note that the neurons in a particular layer are each connected to every output from the previous layer. For this reason, these layers are also referred to as fully-connected layers.</p>
<p>(Technically, the input layer doesn&rsquo;t really use neurons, but it still makes sense to use the notation $a^1$ to denote the input we feed into the first hidden layer).</p>
<p>The input to the neural network is fed into the first of these layers. The outputs of the first layer of neurons is computed and passed onto the next layer. This continues until we reach the output layer of neurons. With a well-trained network, we should see the outputs from the final layer can easily be &ldquo;interpreted&rdquo; to decide on a correct label for our input.</p>
<p>This is still a bit too imprecise! Let&rsquo;s work through how this would work for a classifier on the MNIST handwritten digit dataset.</p>
<p>Due to the &ldquo;flatness&rdquo; of the layers, we first flatten the 28x28 image into a list of length 784 to pass into the network (in practice we would also normalise the pixel values to lie between 0 and 1). This is the input to our network. The output of the network should be a layer containing 10 neurons, which should each correspond to a label (from 0-9).</p>
<p>We will train the network a way such that the label which the network chooses is decided from the largest output from the output layer neurons. In an ideal case (remembering the range of the sigmoid function), we would expect to see a &ldquo;1&rdquo; output from the neuron matching the correct label, and a &ldquo;0&rdquo; everywhere else.</p>
<p><img src="show_example" alt="Placeholder"></p>
<hr>
<h2 id="defining-our-notation">
  Defining our notation
  <a class="heading-link" href="#defining-our-notation">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Going back to a more general case, suppose we have a neural network consisting of some layers of neurons. We will use $L$ to denote the number of layers, and label them from 1 to $L$ inclusive.</p>
<p>We will let $z^l_i$ refer to the weighted input to the $i^{\text{th}}$ neuron in the $l^{\text{th}}$ layer and $a^l_i$ to refer to that same neuron&rsquo;s output after applying the activation function, so in particular, we have that:</p>
$$a^l_i=\sigma(z^l_i)$$<p>We now introduce the notation for the weights and biases. We let:</p>
$$w^l_{i,j}$$<p>refer to the weight for the $i^{\text{th}}$ neuron in the $l^{\text{th}}$ layer coming from the $j^{\text{th}}$ output from the $l-1^{\text{th}}$ layer.</p>
<p>We let:</p>
$$b^l_i$$<p>refer to the bias on the $i^{\text{th}}$ neuron in the $l^{\text{th}}$ layer.</p>
<p>Using this notation, we can now express the weighted input to any neuron in the network (where m is the number of neurons in the previous layer):</p>
$$z^l_i = w^l_{i,1}a^{l-1}_1 + .… + w^l_{i,m}a^{l-1}_m+b^l_i$$<p>Using the first subscript to denote which neuron the weight is connected to in the &ldquo;later&rdquo; layer may seem quite strange at first, but it turns out that it makes everything a lot neater, as we will see now.</p>
<p>You may have noticed that if we pack together the weighted sums of inputs into a vector, it pretty much looks like the result of some matrix-vector product.</p>
<p>In fact, looking at the weighted inputs to the $l^{\text{th}}$ layer (before applying the bias), we have:</p>
$$
\begin{pmatrix}
w^l_{1,1}a^{l-1}_1+~...~+w^l_{1,m}a^{l-1}_m\\
w^l_{2,1}a^{l-1}_1+~...~+w^l_{2,m}a^{l-1}_m\\
\vdots \\
w^l_{n,1}a^{l-1}_1+~...~+w^l_{n,m}a^{l-1}_m
\end{pmatrix}=
\begin{pmatrix}
w^l_{1,1} & w^l_{1,2} & \dots & w^l_{1,m} \\
w^l_{2 ,1} & w^l_{2,2} & & \vdots \\
\vdots & & \ddots & \vdots \\
w^l_{n,1} & \dots & \dots & w^l_{n,m}
\end{pmatrix}
\begin{pmatrix}
a^{l-1}_1 \\
a^{l-1}_2 \\
\vdots \\
a^{l-1}_m \\
\end{pmatrix}$$<p>where $m$ is the number of neurons in the $l-1^{\text{th}}$ layer and $n$ is the number of neurons in the $l^{\text{th}}$ layer.</p>
<p>This is why our &ldquo;backwards&rdquo; subscripts on the weights are nice - we can express the forwards pass as a matrix multiplication with a <strong>weight matrix</strong>, where the weights are packed into the matrix with the standard method for indexing.</p>
<p>In practice, it is useful to write this computation as a matrix multiplication as libraries with linear algebra capabilities are often optimised to perform these operations very quickly.</p>
<p>If we define the weights between layers $l$ and $l-1$ (which have $n$ and $m$ neurons like before):</p>
$$w^l=\begin{pmatrix}
w^l_{1,1} & w^l_{1,2} & \dots & w^l_{1,m} \\
w^l_{2 ,1} & w^l_{2,2} & & \vdots \\
\vdots & & \ddots & \vdots \\
w^l_{n,1} & \dots & \dots & w^l_{n,m}
\end{pmatrix}$$<p>and the biases for layer $l$:</p>
$$b^l = \begin{pmatrix}
b^l_1 \\
b^l_2 \\
\vdots \\
b^l_n
\end{pmatrix}$$<p>We now have a very compact way to express the weighted inputs to a layer given the outputs from a previous layer:</p>
$$z^l=w^la^{l-1}+b^l$$<p>If for $z$ a matrix/vector we define $\sigma(z)$ to mean applying $\sigma$ to each individual entry in $z$, we have:</p>
$$a^l=\sigma(z^l)=\sigma(w^la^{l-1}+b^l)$$<p>Now, given an input $x=a^1$, we can repeatedly apply this above expression (as $l$ ranges from 2 to $L$) to get an output. This is known as a <strong>forward pass</strong> through the network. We now need to know how to modify the weights and biases in the network to allow it to succeed in classifying objects.</p>
<hr>
<h2 id="the-cost-function">
  The cost function
  <a class="heading-link" href="#the-cost-function">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>The cost function is a measure of how &ldquo;far away&rdquo; our network is from its desired behaviour.</p>
<p>We let $x$ be an input to our network, and $y$ to be the desired output. For MNIST, this desired output is constructed from the label which is included with the dataset for each image.</p>
<p>More precisely, if the image label is $i$, where $0\le i\le9$, then the desired output of the network $y$ is a column vector of length 10, with a &ldquo;1&rdquo; in the $i^{\text{th}}$ position and &ldquo;0&quot;s everywhere else.</p>
<p>For example, if the label is &ldquo;3&rdquo;, then we would have:</p>
$$y=\begin{pmatrix}
0 \\
0 \\
0 \\
1 \\
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
\end{pmatrix}$$<p>Recalling that the output of our network is $a^L$ (which is also clearly dependent on $x$), the cost function should be a measure of similarity between $y$ and $a^L$. It should be large when they are very different, and small when they are very similar.</p>
<p>A very natural choice is the squared Euclidean distance between $y$ and $a^L$ (treating them as vectors in $\mathbb{R}^n$ where $n$ is the size of the output layer):</p>
$$C=\sum{(y_i-a^L_i)^2}$$<p>Training the network using this <strong>quadratic cost</strong> does work, but in practice, another cost function called the <strong>cross entropy loss</strong> works better with sigmoid neurons for reasons we will get into later.</p>
<p>In our MNIST example, the training data consists of a large number of examples. The goal of training can now be restated more formally - we aim to change the weights and biases in the network to <strong>minimise</strong> the cost function:</p>
$$C=\frac{1}{n}\sum_{\forall x}C_x$$<p>Here, each $C_x$ is the cost from each training example (which is clearly only dependent on $x$) and $n$ is the number of training examples. We are trying to minimise the average loss from all of the examples.</p>
<hr>
<h2 id="gradient-descent-and-sgd">
  Gradient descent and SGD
  <a class="heading-link" href="#gradient-descent-and-sgd">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Recall that given some function $f$ in multiple independent variables $x_1,x_2,&hellip;,x_n$, the gradient vector is given by:</p>
$$\nabla f(x_1,...,x_n)=\begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{pmatrix}$$<p>At some given point $x=(x_1,&hellip;,x_n)$, the gradient vector is the direction of greatest increase of $f$ (at least locally). The direction of greatest <em>decrease</em> in $f$ is in the opposite direction to the gradient vector. This is what we will be using in gradient descent.</p>
<p>Both of these results can be derived from the fact that locally, at a given point $x_0=(x_1,&hellip;,x_n)$, $f$ can be approximated by a &ldquo;tangent&rdquo; plane:</p>
$$f(x)\approx f(x_0)+\nabla f \cdot (x-x_0)$$<p>where $x$ represents a general point in $n$-dimensional space.</p>
<p>This dot product (given the size of the step $\left\lVert x-x_0\right\rVert$ is held constant) is maximised precisely when the displacement of a point from the initial position $x_0$, given by $x-x_0$, points exactly in the direction of the gradient vector, and minimised exactly when $x-x_0$ points in the opposite direction to the gradient vector.</p>
<p>The direction of the gradient vector for a general function is going to change as we change its inputs. The idea of <strong>gradient descent</strong> is to take small steps in the opposite direction of the gradient vector, so we end up taking a &ldquo;downhill&rdquo; path to points where the function is smaller.</p>
<p>This is an iterative process, where we repeatedly modify the input to the function $x=(x_1,&hellip;,x_n)$ according to the following formula:</p>
$$x\rightarrow x-\eta\nabla f$$<p>$x$ is getting updated by making a small step in the opposite direction of the gradient vector ($-\eta\nabla f$). Here, $\eta$ is a parameter which controls how large the step size is, known as the <strong>learning rate</strong>.</p>
<p>In our case, we can treat our cost function $C$ as being a function of all of the weights and biases in the network. This is because we are already fixing all of our various inputs to the network during training (the $x$&rsquo;s), and it is now up to us to modify the parameters.</p>
<p>Following our gradient descent formula above, looking at the weights and biases individually gives the following formulae for updates (for a single weight/bias):</p>
$$w_k\rightarrow w_k-\eta\frac{\partial C}{\partial w_k}$$<p>
</p>
$$b_k\rightarrow b_k-\eta\frac{\partial C}{\partial b_k}$$<p>Recall that we sum over the costs for each input to define the final cost function $C$:</p>
$$C=\sum_{\forall x}C_x$$<p>By the linearity of derivatives, for any weight or bias, we have:
</p>
$$\frac{\partial C}{\partial w_k}=\frac{1}{n}\sum_{\forall x}\frac{\partial C_x}{\partial w_k}$$<p>
</p>
$$\frac{\partial C}{\partial b_k}=\frac{1}{n}\sum_{\forall x}\frac{\partial C_x}{\partial b_k}$$<p>In other words, we can just compute the cost gradients for each training example and sum over all of them to compute the final gradients for each parameter to be used in the update.</p>
<p>In practice however, this is painfully inefficient. For just a single step, we require <em>the entire dataset to be processed</em>. Keep in mind that even MNIST has 60,000 training images.</p>
<p>Instead of computing gradients for all of the training data, we instead do it in small batches. The idea is that (after some rescaling) this <em>approximation</em> of the cost function will result in gradients for the parameters which are roughly in the same &ldquo;direction&rdquo; as if we used all of the training data.</p>
<p>To be a bit more precise, let $x_1,&hellip;,x_m$ be a sampled mini-batch of inputs, where $m$ is the mini-batch size. As an example, I used $m=20$ when training my network. We then approximate the derivatives of $C$ by taking an average over the gradients from our mini-batch:</p>
$$\frac{\partial C}{\partial w_k}\approx\frac{1}{m}\sum_{i=1}^m\frac{\partial C_i}{\partial w_k}$$<p>
</p>
$$\frac{\partial C}{\partial b_k}\approx\frac{1}{m}\sum_{i=1}^m\frac{\partial C_i}{\partial b_k}$$<p>In practice, this works incredibly well and even helps the gradient descent method escape from local minima. This technique of using mini-batches to update gradients is known as <strong>stochastic gradient descent (SGD)</strong>.</p>
<p>I&rsquo;ve missed out a pretty important detail - how are we even meant to compute the partial derivatives of the cost w.r.t. the various weights and biases in the network? After all, the layered structure of the network seems to make things very complicated.</p>
<p>It turns out that there is an algorithm called <strong>backpropagation</strong> which allows us to do these gradient computations very quickly in a very understandable and elegant way!</p>
<hr>
<h2 id="backpropagation">
  Backpropagation
  <a class="heading-link" href="#backpropagation">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>The goal of backpropagation is to compute the partial derivatives of the cost function for a single training example w.r.t. the parameters in the network.</p>
<p>For this section, I will use $C$ to refer to the cost for a single training example (as opposed to earlier where it was used to denote the average cost across all training examples), and we will minimise $C$ as a function of the parameters of the network (we obviously don&rsquo;t get to tamper with the training data!).</p>
<p>I mentioned before that the <strong>weighted input</strong> to a neuron $z^l_i$ would be useful later. We define the <strong>error</strong> of the $i^{\text{th}}$ neuron in the $l^{\text{th}}$ layer as follows:</p>
$$\delta^l_i=\frac{\partial C}{\partial z^l_i}$$<p>Using a similar notation to before, we will let:</p>
$$\delta^l$$<p>refer to a vector containing all of the errors in a given layer.</p>
<p>It turns out that to compute the gradients for the weights and biases, computing the &ldquo;intermediate&rdquo; error term simplifies things a lot!</p>
<p>Before we continue, there is an important notation we need to introduce:</p>
<hr>
<h3 id="the-hadamard-product">
  The Hadamard product
  <a class="heading-link" href="#the-hadamard-product">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The <strong>Hadamard product</strong> (also called the Schur product) is an an operation defined between 2 matrices or 2 vectors with the same dimensions, which maps it to a matrix/vector of the same dimensions as the input. If $a$ and $b$ are our 2 inputs, their Hadamard product is denoted:</p>
$$a \odot b$$<p>To compute it, we multiply each element in the first matrix/vector with its corresponding element in the second matrix/vector. To illustrate with an example, let:</p>
$$a=\begin{pmatrix}
1 \\
2 \\
3\end{pmatrix},
b=\begin{pmatrix}
4 \\
5 \\
6\end{pmatrix}$$<p>Then:</p>
$$a\odot b=
\begin{pmatrix}
1\times 4 \\
2\times 5 \\
3\times 6\end{pmatrix}
=\begin{pmatrix}
4 \\
10 \\
18\end{pmatrix}$$<hr>
<p>We can now outline our plan to compute the gradients of the cost w.r.t. the weights and biases. We first compute the errors of all the neurons, and it turns out that there is a very neat way to do this. Then, once we have computed the errors, we can compute the weight and bias gradients from them nicely. This completes our goal - now these gradients can be used in the gradient descent process to update the parameters of the network to lower the cost function.</p>
<p>This entire process can be condensed into <strong>4 equations</strong>, which we will derive now. If you are comfortable with the chain rule for multiple variables, I encourage you to try deriving these on your own.</p>
<hr>
<h3 id="1-errors-in-the-output-layer">
  1: Errors in the output layer
  <a class="heading-link" href="#1-errors-in-the-output-layer">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The errors in the output layer $\delta^L$ are clearly going to dependent on our choice of cost function. We claim that:</p>
$$\delta^L=\nabla_a C\odot \sigma'(z^L)$$<p>Here, we are treating $C$ as a function of only the outputs of the output layer of neurons $a^L$. This means that $\nabla_a C$ is a gradient vector &ldquo;with respect to&rdquo; the outputs of the network. To be more concrete, we have (where $n$ is the number of output neurons):</p>
$$\nabla_a C=\begin{pmatrix}
\frac{\partial C}{\partial a^L_1} \\
\vdots \\
\frac{\partial C}{\partial a^L_n}\end{pmatrix}$$<h4 id="proof">
  Proof
  <a class="heading-link" href="#proof">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>We treat $C$ as a function of the various outputs in $a^L$, $C=C(a^L_1,&hellip;,a^L_n)$. By chain rule, we have:</p>
$$\frac{\partial C}{\partial z^L_k}=\sum_{i=1}^n\frac{\partial C}{\partial a^L_i}\frac{\partial a^L_i}{\partial z^L_k}$$<p>However, notice that when $k\ne i$, the partial derivative:</p>
$$\frac{\partial a^L_i}{\partial z^L_k}$$<p>ends up vanishing as this output $a^L_i$ is independent of the weighted input $z^L_k$ which is at a different neuron. Now we are only left with the term in the sum with $i=k$, which leaves:</p>
$$\frac{\partial C}{\partial z^L_k}=\frac{\partial C}{\partial a^L_k}\frac{d a^L_k}{d z^L_k}$$<p>where it&rsquo;s fine to write the second term is an ordinary derivative as we have already defined $a^L_k=\sigma(z^L_k)$. Rewriting in our notation:</p>
$$\delta^L_k=\frac{\partial C}{\partial a^L_k}\sigma'(z^L_k)$$<p>for each $k$. This is a formula for the individual components of our error vector and it can now be easily verified that the formula which we claimed above is true.</p>
<h4 id="output-layer-errors-with-quadratic-loss">
  Output layer errors with quadratic loss
  <a class="heading-link" href="#output-layer-errors-with-quadratic-loss">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>Again, if you haven&rsquo;t done so before, I would encourage you to try deriving this result on your own first. Recall that given a target label $y$ and the output of our network $a^L$, our quadratic loss was defined by:</p>
$$\sum_{i=1}^n(y_i-a^L_i)^2$$<p>For any particular $a^L_k$, we have:</p>
$$\frac{\partial C}{\partial a^L_k}=2(a^L_i-y_i)$$<p>It is not hard now to verify that in the case of quadratic loss,</p>
$$\nabla_a C=2(a^L-y)$$<p>Subbing back in:</p>
$$\delta^L=2(a^L-y)\odot \sigma'(z^L)$$<hr>
<h3 id="2-errors-in-a-layer-from-errors-in-the-next-layer">
  2: Errors in a layer from errors in the next layer
  <a class="heading-link" href="#2-errors-in-a-layer-from-errors-in-the-next-layer">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>We claim that (at least for $l&lt;L$):</p>
$$\delta^l=((w^{l+1})^T\delta^{l+1})\odot\sigma'(z^l)$$<p>Here, the $T$ denotes a matrix transpose. Note that as $w^{l+1}a^l$ is used to compute $a^{l+1}$, their shapes are the same. The errors in a layer must have the same shape as the outputs. In other words, we have that $w^{l+1}\delta^l$ has the same shape as $\delta^{l+1}$. This also tells us that $\delta^l$ and $(w^{l+1})^T\delta^{l+1}$ have the same shape, which at least verifies that this formula is &ldquo;dimensionally&rdquo; correct.</p>
<h4 id="proof-1">
  Proof
  <a class="heading-link" href="#proof-1">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>Here, we will treat $C=C(z^{l+1}_1,&hellip;,z^{l+1}_n)$ as a function of the $n$ weighted inputs to the $(l+1)^{\text{th}}$ layer. This is a natural choice, as we will get the nice $\delta^{l+1}_i$-type terms when we start applying the chain rule. Doing just that:</p>
$$\frac{\partial C}{\partial z^l_k}=\sum_{i=1}^n\frac{\partial C}{\partial z^{l+1}_i}\frac{\partial z^{l+1}_i}{\partial z^l_k}$$<p>The second term </p>
$$\frac{\partial z^{l+1}_i}{\partial z^l_k}$$<p> can be chain-ruled again by using $a^l_k$ (noting all of the other partial derivatives which may appear vanish for the same reason as in our previous proof), which gives:</p>
$$\frac{\partial C}{\partial z^l_k}=\sum_{i=1}^n\frac{\partial C}{\partial z^{l+1}_i}\frac{\partial z^{l+1}_i}{\partial a^l_k}\frac{d a^l_k}{d z^l_k}$$<p>Now recalling how $z^{l+1}_i$ is a weighted input of the $m$ various $a^l_j$s:</p>
$$z^{l+1}_i=\sum_{j=1}^m(w^{l+1}_{i,j}a^l_j)+b^{l+1}_i$$<p>We can compute the partial derivative $\frac{\partial z^{l+1}_i}{\partial a^l_k}$ in our sum:</p>
$$\frac{\partial z^{l+1}_i}{\partial a^l_k}=w^{l+1}_{i,k}$$<p>as all of the other terms are independent of $a^l_k$ so taking partial derivatives causes them to vanish. Subbing this back in and rewriting the other 2 partial derivatives in the summand in a more useful way:</p>
$$\frac{\partial C}{\partial z^l_k}=\sum_{i=1}^n\delta^{l+1}_i w^{l+1}_{i,k}\sigma'(z^l_k)$$<p>And factorising out the $\sigma&rsquo;$ term which is held constant throughout the sum for neatness:</p>
$$\frac{\partial C}{\partial z^l_k}=\sigma'(z^l_k)\sum_{i=1}^n\delta^{l+1}_i w^{l+1}_{i,k}$$<p>This is a formula for the components of the error vector in the $l^{\text{th}}$ layer. This &ldquo;collection&rdquo; of sums can be represented concisely as a matrix product (the formula above), which I leave you to verify :)</p>
<hr>
<h3 id="3-bias-gradients-from-errors">
  3: Bias gradients from errors
  <a class="heading-link" href="#3-bias-gradients-from-errors">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>We claim (for $l\ge 2$ as the input layer doesn&rsquo;t have any parameters attached to it):</p>
$$\frac{\partial C}{\partial b^l_j}=\delta^l_j$$<p>We haven&rsquo;t really defined any notation yet for a vector storing the bias gradients. The cost can&rsquo;t be written to only be dependent on the biases in a single layer, which makes a gradient vector notation seem wrong.</p>
<h4 id="proof-2">
  Proof
  <a class="heading-link" href="#proof-2">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>Thankfully, this is probably the formula with the shortest proof. We consider the weighted input $z^l_j$, which is given by the formula (where $n$ is the number of neurons in the $l-1^{\text{th}}$ layer):</p>
$$z^l_j=\sum_{i=1}^n(w^l_{j,i}a^{l-1}_i)+b^l_j$$<p>Clearly, the partial derivative $\frac{\partial z^l_j}{\partial b^l_j}$ is equal to 1. By chain rule again, treating $C=C(z^l_1,&hellip;,z^l_m)$ as a function of the various $z^l_i$s and discarding the terms in the sum which have a partial derivative which vanishes from being independent of this bias, we get:</p>
$$\frac{\partial C}{\partial b^l_j}=1\times\frac{\partial C}{\partial z^l_j}=\delta^l_j$$<p>which is the desired result.</p>
<hr>
<h3 id="4-weight-gradients-from-errors">
  4: Weight gradients from errors
  <a class="heading-link" href="#4-weight-gradients-from-errors">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>We claim:
</p>
$$\frac{\partial C}{\partial w^l_{j,k}}=a^{l-1}_k\delta^l_j$$<h4 id="proof-3">
  Proof
  <a class="heading-link" href="#proof-3">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>Again, let&rsquo;s treat $C=C(z^l_1,&hellip;,z^l_m)$ as a function of the various $z^l_i$s. Using:</p>
$$z^l_j=\sum_{i=1}^n(w^l_{j,i}a^{l-1}_i)+b^l_j$$<p>again, differentiating w.r.t. $w^l_{j,k}$ gives (after ignoring vanishing terms):</p>
$$\frac{\partial z^l_j}{\partial w^l_{j,k}}=a^{l-1}_k$$<p>Using the chain rule similarly above gives:</p>
$$\frac{\partial C}{\partial w^l_{j,k}}=\frac{\partial C}{\partial z^l_j}\frac{\partial z^l_j}{\partial w^l_{j,k}}=\delta^l_j a^{l-1}_k$$<p>which is the desired result.</p>
<hr>
<h3 id="in-summary">
  In summary
  <a class="heading-link" href="#in-summary">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>We have the following 4 equations:</p>
$$
\begin{align}
& \delta^L=\nabla_a C\odot \sigma'(z^L) \\
& \delta^l=((w^{l+1})^T\delta^{l+1})\odot\sigma'(z^l) \\
& \frac{\partial C}{\partial b^l_j}=\delta^l_j \\
& \frac{\partial C}{\partial w^l_{j,k}}=a^{l-1}_k\delta^l_j
\end{align}
$$<hr>
<h2 id="outlining-the-training-process">
  Outlining the training process
  <a class="heading-link" href="#outlining-the-training-process">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>We now have backpropagation as a way to compute gradients, so we are ready to outline how the network can be trained. It can be summarised in  a few steps:</p>
<ul>
<li>Randomly generate minibatches of training examples (of size $m$).</li>
<li>For each minibatch of training examples $x_1,x_2,&hellip;,x_m$:
<ul>
<li>Pass each $x_i$ forwards through the network.
<ul>
<li>Set $a^1=x_i$</li>
<li>Repeatedly compute $a^{l+1}=\sigma(w^{l+1}a^l+b^{l+1})$ until the output layer.</li>
</ul>
</li>
<li>Backpropagate the loss:
<ul>
<li>Compute output layer errors:
<ul>
<li>$\delta^L=\nabla_a C\odot \sigma&rsquo;(z^L)$</li>
</ul>
</li>
<li>Compute errors in previous layers:
<ul>
<li>$\delta^l=((w^{l+1})^T\delta^{l+1})\odot\sigma&rsquo;(z^l)$</li>
</ul>
</li>
<li>Compute weight and bias gradients:
<ul>
<li>$\frac{\partial C}{\partial b^l_j}=\delta^l_j$</li>
<li>$\frac{\partial C}{\partial w^l_{j,k}}=a^{l-1}_k\delta^l_j$</li>
</ul>
</li>
<li>Accumulate this loss over all of the examples in the minibatch</li>
</ul>
</li>
<li>Update parameters using the averaged gradients (after dividing by $m$) with gradient descent.
<ul>
<li>Done by using the updates:
<ul>
<li>$w_k\rightarrow w_k-\frac{\eta}{m}\frac{\partial C}{\partial w_k}$</li>
<li>$b_k\rightarrow b_k-\frac{\eta}{m}\frac{\partial C}{\partial b_k}$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We will go over how this can actually be implemented later :)</p>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script>
  window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "yourdiscussshortname" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    
    document.addEventListener('themeChanged', function (e) { 
        if (document.readyState == 'complete') {
          DISQUS.reset({ reload: true, config: disqus_config });
        }
    });
</script>
        
        
        
        
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2025
     Ben Chang 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
